{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch transformers datasets soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Paths to your data\n",
    "mp3_dir = r\"C:\\Users\\alexx\\OneDrive\\Escriptori\\uni\\4th year\\Advanced Machine Learning\\project\\cv-corpus-18.0-delta-2024-06-14-ca\\cv-corpus-18.0-delta-2024-06-14\\ca\\clips\"\n",
    "wav_dir = r\"C:\\Users\\alexx\\OneDrive\\Escriptori\\uni\\4th year\\Advanced Machine Learning\\project\\cv-corpus-18.0-delta-2024-06-14-ca\\cv-corpus-18.0-delta-2024-06-14\\ca\\wav\"\n",
    "\n",
    "# Iterate over all mp3 files and convert them to wav\n",
    "for file_name in os.listdir(mp3_dir):\n",
    "    if file_name.endswith(\".mp3\"):\n",
    "        mp3_path = os.path.join(mp3_dir, file_name)\n",
    "        wav_path = os.path.join(wav_dir, os.path.splitext(file_name)[0] + \".wav\")\n",
    "\n",
    "        # Load mp3 file\n",
    "        audio = AudioSegment.from_mp3(mp3_path)\n",
    "        \n",
    "        # Export as wav file with the desired sample rate (16kHz)\n",
    "        audio.export(wav_path, format=\"wav\", parameters=[\"-ar\", \"16000\"])  # \"-ar\" specifies sample rate\n",
    "        \n",
    "print(\"Conversion completed!\")\n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, HubertForCTC, TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset path\n",
    "dataset_path = r\"C:\\Users\\alexx\\OneDrive\\Escriptori\\uni\\4th year\\Advanced Machine Learning\\project\\cv-corpus-18.0-delta-2024-06-14-ca\\cv-corpus-18.0-delta-2024-06-14\\ca\\wav\"\n",
    "\n",
    "# Load TSV files for Common Voice\n",
    "validated_tsv_path = r\"C:\\Users\\alexx\\OneDrive\\Escriptori\\uni\\4th year\\Advanced Machine Learning\\project\\cv-corpus-18.0-delta-2024-06-14-ca\\cv-corpus-18.0-delta-2024-06-14\\ca\\validated.tsv\"\n",
    "\n",
    "# Change the extension of each file in valida_tsv_path to .wav\n",
    "validated_df = pd.read_csv(validated_tsv_path, sep='\\t')\n",
    "validated_df['path'] = validated_df['path'].apply(lambda x: os.path.splitext(x)[0] + '.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct paths to point to the full audio file paths\n",
    "validated_df['path'] = validated_df['path'].apply(lambda x: os.path.join(dataset_path, x))\n",
    "\n",
    "# Create Hugging Face dataset from the data frame\n",
    "dataset = Dataset.from_pandas(validated_df)\n",
    "\n",
    "# Add audio column\n",
    "dataset = dataset.cast_column(\"path\", Audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load HuBERT processor and model\n",
    "model_name = \"facebook/hubert-large-ls960-ft\"  # You can use other variants of HuBERT as well\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = HubertForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(batch):\n",
    "    # Load audio and prepare input features\n",
    "    audio = batch[\"path\"][\"array\"]\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2884 [00:00<?, ? examples/s]c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2884/2884 [00:43<00:00, 65.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocess function to the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hubert-finetuned-catalan\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Turn true when GPU avilable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "class CustomDataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor: Wav2Vec2Processor, padding: Union[bool, str] = True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split input values and labels\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input values\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Pad labels, replacing any padding token ids with -100 so they can be ignored during CTC loss computation\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore them during loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Use the custom data collator\n",
    "data_collator = CustomDataCollatorCTCWithPadding(processor=processor, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ncannot import name 'add_model_info_to_custom_pipelines' from 'transformers.utils' (c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1510\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m PYTESSERACT_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the PyTesseract library but it was not found in your environment. You can install it with pip:\u001b[39m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;124m`pip install pytesseract`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m-> 1510\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\image_processing_utils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\image_processing_base.py:29\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature \u001b[38;5;28;01mas\u001b[39;00m BaseBatchFeature\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     IMAGE_PROCESSOR_NAME,\n\u001b[0;32m     31\u001b[0m     PushToHubMixin,\n\u001b[0;32m     32\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     33\u001b[0m     add_model_info_to_custom_pipelines,\n\u001b[0;32m     34\u001b[0m     cached_file,\n\u001b[0;32m     35\u001b[0m     copy_func,\n\u001b[0;32m     36\u001b[0m     download_url,\n\u001b[0;32m     37\u001b[0m     is_offline_mode,\n\u001b[0;32m     38\u001b[0m     is_remote_url,\n\u001b[0;32m     39\u001b[0m     is_vision_available,\n\u001b[0;32m     40\u001b[0m     logging,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'add_model_info_to_custom_pipelines' from 'transformers.utils' (c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load metrics\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m      4\u001b[0m wer_metric \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\evaluate\\__init__.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m SCRIPTS_VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(__version__)\u001b[38;5;241m.\u001b[39mis_devrelease \u001b[38;5;28;01melse\u001b[39;00m __version__\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_suite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     AudioClassificationEvaluator,\n\u001b[0;32m     32\u001b[0m     AutomaticSpeechRecognitionEvaluator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     evaluator,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m push_to_hub\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DownloadConfig, DownloadMode, load_dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluation_module_factory\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\evaluate\\evaluator\\__init__.py:27\u001b[0m\n\u001b[0;32m     23\u001b[0m     TRANSFORMERS_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioClassificationEvaluator\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautomatic_speech_recognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutomaticSpeechRecognitionEvaluator\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\evaluate\\evaluator\\audio_classification.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationModule\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_end_docstrings, add_start_docstrings\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionMixin, Pipeline, PreTrainedModel, PreTrainedTokenizer, TFPreTrainedModel\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\evaluate\\evaluator\\base.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, pipeline\n\u001b[0;32m     36\u001b[0m     TRANSFORMERS_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1500\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1493\u001b[0m NLTK_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the NLTK library but it was not found in your environment. You can install it by referring to:\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;124mhttps://www.nltk.org/install.html. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[1;32m-> 1500\u001b[0m VISION_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the PIL library but it was not found in your environment. You can install it with pip:\u001b[39m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124m`pip install pillow`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m PYTESSERACT_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the PyTesseract library but it was not found in your environment. You can install it with pip:\u001b[39m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;124m`pip install pytesseract`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1512\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1506\u001b[0m # docstyle-ignore\n\u001b[0;32m   1507\u001b[0m PYTESSERACT_IMPORT_ERROR = \"\"\"\n\u001b[0;32m   1508\u001b[0m {0} requires the PyTesseract library but it was not found in your environment. You can install it with pip:\n\u001b[0;32m   1509\u001b[0m `pip install pytesseract`. Please note that you may need to restart your runtime after installation.\n\u001b[0;32m   1510\u001b[0m \"\"\"\n\u001b[1;32m-> 1512\u001b[0m # docstyle-ignore\n\u001b[0;32m   1513\u001b[0m PYCTCDECODE_IMPORT_ERROR = \"\"\"\n\u001b[0;32m   1514\u001b[0m {0} requires the pyctcdecode library but it was not found in your environment. You can install it with pip:\n\u001b[0;32m   1515\u001b[0m `pip install pyctcdecode`. Please note that you may need to restart your runtime after installation.\n\u001b[0;32m   1516\u001b[0m \"\"\"\n\u001b[0;32m   1518\u001b[0m # docstyle-ignore\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ncannot import name 'add_model_info_to_custom_pipelines' from 'transformers.utils' (c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load metrics\n",
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset,  # You can split the dataset for evaluation separately\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexxsarda\u001b[0m (\u001b[33miauab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\alexx\\OneDrive\\Escriptori\\uni\\4th year\\Advanced Machine Learning\\project\\wandb\\run-20241112_154911-awvxzcq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iauab/huggingface/runs/awvxzcq3' target=\"_blank\">amber-sunset-3</a></strong> to <a href='https://wandb.ai/iauab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iauab/huggingface' target=\"_blank\">https://wandb.ai/iauab/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iauab/huggingface/runs/awvxzcq3' target=\"_blank\">https://wandb.ai/iauab/huggingface/runs/awvxzcq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/900 [00:00<?, ?it/s]c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/900 [16:01<240:04:39, 961.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\alexx\\AppData\\Local\\Temp\\ipykernel_21228\\1814428710.py\", line 2, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 1859, in train\n",
      "    if train_bs_args != train_bs_state:\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 2203, in _inner_training_loop\n",
      "    if self.args.n_gpu > 1:\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 3138, in training_step\n",
      "    if isinstance(self.state.stateful_callbacks[cb_name], list):\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 3161, in compute_loss\n",
      "    if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py\", line 1212, in forward\n",
      "    padded with 0 and passed without `attention_mask`. Be aware that these models also yield slightly different\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py\", line 1088, in forward\n",
      "    if output_attentions:\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py\", line 819, in forward\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py\", line 658, in forward\n",
      "    query_states = query_states.to(target_dtype)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py\", line 466, in forward\n",
      "    key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 167, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
